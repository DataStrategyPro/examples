---
title: "dbplyr"
author: "Jonathan Ng"
date: "2022-08-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(digest)
```

Whenever you're working with data that's in a database or data warehouse it generally makes sense to try and do as much of the processing on the central server as possible before pulling that data into some other analytics tool. 

SQL is the language we use to carry out this central data processing. 

The main problem with SQL especially older variants is that it's extremely repetitive. It's not uncommon to work with tables with hundreds of columns. 

In this article we're going to look at how we can use R to eliminate a lot of the repetition that would occur in the following examples.

1. Moving a column requires rewriting all 100 columns instead of 1
2. Excluding 1 column requires rewriting 99 columns instead of 1
3. Creating a calculation off a calculated column requires 2 queries instead of 1
4. Grouping by 99 columns and summing on 1 (which is often done to consolidate records) requires writing 199 columns instead of 1


## Constructing Sample Data

```{r sample_data}
df <- bind_cols(
matrix(sample(LETTERS[1:5],1000,replace = TRUE), ncol = 10, nrow = 100) %>% 
  as_tibble() %>% 
  set_names(str_c("Category_",1:10)),
matrix(1:1000, ncol = 10, nrow = 100) %>% 
  as_tibble() %>% 
  set_names(str_c("Value_",1:10))
) %>% 
  mutate(key = digest(1:100,algo = "md5"),
         date = ymd(20220101:20220101+100)) %>% 
  relocate(key,date)

df
```

## Add sample to database

```{r cars}
con <- DBI::dbConnect(RSQLite::SQLite(), ":memory:")
copy_to(con, df)

db <- tbl(con, "df")
db
```

## Select Helpers


```{r pressure, echo=FALSE}
db %>% 
  group_by_at(vars(Category_1:Category_10)) %>%
  # group_by_all() %>% 
  summarise_if(is.numeric,sum) %>% 
  relocate(Category_10) %>%
  show_query()
```
